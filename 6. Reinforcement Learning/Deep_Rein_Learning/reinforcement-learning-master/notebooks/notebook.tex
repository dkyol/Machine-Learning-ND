
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Discretization}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Discretization}\label{discretization}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this notebook, you will deal with continuous state and action spaces
by discretizing them. This will enable you to apply reinforcement
learning algorithms that are only designed to work with discrete spaces.

\subsubsection{1. Import the Necessary
Packages}\label{import-the-necessary-packages}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{k+kn}{import} \PY{n+nn}{sys}
         \PY{k+kn}{import} \PY{n+nn}{gym}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         
         \PY{c+c1}{\PYZsh{} Set plotting options}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ggplot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{np}\PY{o}{.}\PY{n}{set\PYZus{}printoptions}\PY{p}{(}\PY{n}{precision}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{120}\PY{p}{)}
\end{Verbatim}


    \subsubsection{2. Specify the Environment, and Explore the State and
Action
Spaces}\label{specify-the-environment-and-explore-the-state-and-action-spaces}

We'll use \href{https://gym.openai.com/}{OpenAI Gym} environments to
test and develop our algorithms. These simulate a variety of classic as
well as contemporary reinforcement learning tasks. Let's use an
environment that has a continuous state space, but a discrete action
space.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{} Create an environment and set random seed}
         \PY{n}{env} \PY{o}{=} \PY{n}{gym}\PY{o}{.}\PY{n}{make}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MountainCar\PYZhy{}v0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{env}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{505}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-yellow}{WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.}

    \end{Verbatim}

    Run the next code cell to watch a random agent.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
         \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{)}\PY{p}{:}
             \PY{n}{action} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
             \PY{n}{env}\PY{o}{.}\PY{n}{render}\PY{p}{(}\PY{p}{)}
             \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
             \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
             \PY{k}{if} \PY{n}{done}\PY{p}{:}
                 \PY{k}{break} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{score}\PY{p}{)}
         \PY{n}{env}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Final score: -200.0

    \end{Verbatim}

    In this notebook, you will train an agent to perform much better! For
now, we can explore the state and action spaces, as well as sample them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{} Explore state (observation) space}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{State space:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{} low:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{low}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{} high:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{high}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
State space: Box(2,)
- low: [-1.2  -0.07]
- high: [0.6  0.07]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{c+c1}{\PYZsh{} Generate some samples from the state space }
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{State space samples:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
State space samples:
[[-1.009e+00  6.313e-02]
 [-7.798e-01  2.657e-02]
 [-1.095e+00  3.230e-02]
 [ 3.871e-01 -3.186e-02]
 [-5.177e-01 -1.760e-02]
 [ 1.478e-01 -3.671e-02]
 [-8.907e-01 -7.099e-03]
 [-6.520e-01  4.749e-02]
 [-7.721e-01  3.345e-04]
 [ 4.967e-01  1.876e-02]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{c+c1}{\PYZsh{} Explore the action space}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Action space:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Generate some samples from the action space}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Action space samples:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Action space: Discrete(3)
Action space samples:
[1 2 1 2 0 2 1 0 0 2]

    \end{Verbatim}

    \subsubsection{3. Discretize the State Space with a Uniform
Grid}\label{discretize-the-state-space-with-a-uniform-grid}

We will discretize the space using a uniformly-spaced grid. Implement
the following function to create such a grid, given the lower bounds
(\texttt{low}), upper bounds (\texttt{high}), and number of desired
\texttt{bins} along each dimension. It should return the split points
for each dimension, which will be 1 less than the number of bins.

For instance, if \texttt{low\ =\ {[}-1.0,\ -5.0{]}},
\texttt{high\ =\ {[}1.0,\ 5.0{]}}, and \texttt{bins\ =\ (10,\ 10)}, then
your function should return the following list of 2 NumPy arrays:

\begin{verbatim}
[array([-0.8, -0.6, -0.4, -0.2,  0.0,  0.2,  0.4,  0.6,  0.8]),
 array([-4.0, -3.0, -2.0, -1.0,  0.0,  1.0,  2.0,  3.0,  4.0])]
\end{verbatim}

Note that the ends of \texttt{low} and \texttt{high} are \textbf{not}
included in these split points. It is assumed that any value below the
lowest split point maps to index \texttt{0} and any value above the
highest split point maps to index \texttt{n-1}, where \texttt{n} is the
number of bins along that dimension.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{k+kn}{import} \PY{n+nn}{math} 
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{k}{def} \PY{n+nf}{create\PYZus{}uniform\PYZus{}grid}\PY{p}{(}\PY{n}{low}\PY{p}{,} \PY{n}{high}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Define a uniformly\PYZhy{}spaced grid that can be used to discretize a space.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    low : array\PYZus{}like}
         \PY{l+s+sd}{        Lower bounds for each dimension of the continuous space.}
         \PY{l+s+sd}{    high : array\PYZus{}like}
         \PY{l+s+sd}{        Upper bounds for each dimension of the continuous space.}
         \PY{l+s+sd}{    bins : tuple}
         \PY{l+s+sd}{        Number of bins along each corresponding dimension.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    grid : list of array\PYZus{}like}
         \PY{l+s+sd}{        A list of arrays containing split points for each dimension.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} TODO: Implement this}
             \PY{n}{lowLow} \PY{o}{=} \PY{n}{low}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{lowHigh} \PY{o}{=} \PY{n}{high}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             
             \PY{n}{interval}  \PY{o}{=} \PY{p}{(}\PY{n}{lowHigh} \PY{o}{\PYZhy{}} \PY{n}{lowLow}\PY{p}{)}\PY{o}{/} \PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
               
             \PY{n}{lowArray} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{val} \PY{o}{=} \PY{n}{lowLow} \PY{o}{+} \PY{n}{interval}
             \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 
                 \PY{n}{lowArray}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{val}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
                 \PY{n}{val} \PY{o}{+}\PY{o}{=} \PY{n}{interval}
                 
             \PY{c+c1}{\PYZsh{}print(lowArray)}
             
             \PY{n}{highLow} \PY{o}{=} \PY{n}{low}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
             \PY{n}{highHigh} \PY{o}{=} \PY{n}{high}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
             
             \PY{n}{interval}  \PY{o}{=} \PY{p}{(}\PY{n}{highHigh} \PY{o}{\PYZhy{}} \PY{n}{highLow}\PY{p}{)}\PY{o}{/} \PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
               
             \PY{n}{highArray} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{val} \PY{o}{=} \PY{n}{highLow} \PY{o}{+} \PY{n}{interval}
             \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 
                 \PY{n}{highArray}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{val}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
                 \PY{n}{val} \PY{o}{+}\PY{o}{=} \PY{n}{interval}
                 
             \PY{c+c1}{\PYZsh{}print(highArray)}
             
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{lowArray}\PY{p}{,} \PY{n}{highArray}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{low} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{5.0}\PY{p}{]}
         \PY{n}{high} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}
         \PY{n}{create\PYZus{}uniform\PYZus{}grid}\PY{p}{(}\PY{n}{low}\PY{p}{,} \PY{n}{high}\PY{p}{)}  \PY{c+c1}{\PYZsh{} [test]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}86}]:} array([[-0.8, -0.6, -0.4, -0.2, -0. ,  0.2,  0.4,  0.6,  0.8],
                [-4. , -3. , -2. , -1. ,  0. ,  1. ,  2. ,  3. ,  4. ]])
\end{Verbatim}
            
    Now write a function that can convert samples from a continuous space
into its equivalent discretized representation, given a grid like the
one you created above. You can use the
\href{https://docs.scipy.org/doc/numpy-1.9.3/reference/generated/numpy.digitize.html}{\texttt{numpy.digitize()}}
function for this purpose.

Assume the grid is a list of NumPy arrays containing the following split
points:

\begin{verbatim}
[array([-0.8, -0.6, -0.4, -0.2,  0.0,  0.2,  0.4,  0.6,  0.8]),
 array([-4.0, -3.0, -2.0, -1.0,  0.0,  1.0,  2.0,  3.0,  4.0])]
\end{verbatim}

Here are some potential samples and their corresponding discretized
representations:

\begin{verbatim}
[-1.0 , -5.0] => [0, 0]
[-0.81, -4.1] => [0, 0]
[-0.8 , -4.0] => [1, 1]
[-0.5 ,  0.0] => [2, 5]
[ 0.2 , -1.9] => [6, 3]
[ 0.8 ,  4.0] => [9, 9]
[ 0.81,  4.1] => [9, 9]
[ 1.0 ,  5.0] => [9, 9]
\end{verbatim}

\textbf{Note}: There may be one-off differences in binning due to
floating-point inaccuracies when samples are close to grid boundaries,
but that is alright.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{k}{def} \PY{n+nf}{discretize}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{grid}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Discretize a sample as per given grid.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    sample : array\PYZus{}like}
         \PY{l+s+sd}{        A single sample from the (original) continuous space.}
         \PY{l+s+sd}{    grid : list of array\PYZus{}like}
         \PY{l+s+sd}{        A list of arrays containing split points for each dimension.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    discretized\PYZus{}sample : array\PYZus{}like}
         \PY{l+s+sd}{        A sequence of integers with the same number of dimensions as sample.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} TODO: Implement this}
           
             \PY{n}{Xindices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{digitize}\PY{p}{(}\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{grid}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
           
             \PY{n}{Yindices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{digitize}\PY{p}{(}\PY{n}{sample}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{grid}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{Xindices}\PY{p}{,} \PY{n}{Yindices}\PY{p}{)}\PY{p}{)}
             
         
         
         \PY{c+c1}{\PYZsh{} Test with a simple grid and some samples}
         \PY{n}{grid} \PY{o}{=} \PY{n}{create\PYZus{}uniform\PYZus{}grid}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{)}
         \PY{n}{samples} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}
             \PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0} \PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{,}
              \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.81}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{4.1}\PY{p}{]}\PY{p}{,}
              \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.8} \PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{,}
              \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5} \PY{p}{,}  \PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{,}
              \PY{p}{[} \PY{l+m+mf}{0.2} \PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.9}\PY{p}{]}\PY{p}{,}
              \PY{p}{[} \PY{l+m+mf}{0.8} \PY{p}{,}  \PY{l+m+mf}{4.0}\PY{p}{]}\PY{p}{,}
              \PY{p}{[} \PY{l+m+mf}{0.81}\PY{p}{,}  \PY{l+m+mf}{4.1}\PY{p}{]}\PY{p}{,}
              \PY{p}{[} \PY{l+m+mf}{1.0} \PY{p}{,}  \PY{l+m+mf}{5.0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n}{discretized\PYZus{}samples} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{discretize}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{grid}\PY{p}{)} \PY{k}{for} \PY{n}{sample} \PY{o+ow}{in} \PY{n}{samples}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Samples:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{repr}\PY{p}{(}\PY{n}{samples}\PY{p}{)}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Discretized samples:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{repr}\PY{p}{(}\PY{n}{discretized\PYZus{}samples}\PY{p}{)}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Samples:
array([[-1.  , -5.  ],
       [-0.81, -4.1 ],
       [-0.8 , -4.  ],
       [-0.5 ,  0.  ],
       [ 0.2 , -1.9 ],
       [ 0.8 ,  4.  ],
       [ 0.81,  4.1 ],
       [ 1.  ,  5.  ]])

Discretized samples:
array([[0, 0],
       [0, 0],
       [1, 1],
       [2, 5],
       [6, 3],
       [9, 9],
       [9, 9],
       [9, 9]], dtype=int64)

    \end{Verbatim}

    \subsubsection{4. Visualization}\label{visualization}

It might be helpful to visualize the original and discretized samples to
get a sense of how much error you are introducing.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{collections} \PY{k}{as} \PY{n+nn}{mc}
         
         \PY{k}{def} \PY{n+nf}{visualize\PYZus{}samples}\PY{p}{(}\PY{n}{samples}\PY{p}{,} \PY{n}{discretized\PYZus{}samples}\PY{p}{,} \PY{n}{grid}\PY{p}{,} \PY{n}{low}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{high}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Visualize original and discretized samples on a given 2\PYZhy{}dimensional grid.\PYZdq{}\PYZdq{}\PYZdq{}}
         
             \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Show grid}
             \PY{n}{ax}\PY{o}{.}\PY{n}{xaxis}\PY{o}{.}\PY{n}{set\PYZus{}major\PYZus{}locator}\PY{p}{(}\PY{n}{plt}\PY{o}{.}\PY{n}{FixedLocator}\PY{p}{(}\PY{n}{grid}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{yaxis}\PY{o}{.}\PY{n}{set\PYZus{}major\PYZus{}locator}\PY{p}{(}\PY{n}{plt}\PY{o}{.}\PY{n}{FixedLocator}\PY{p}{(}\PY{n}{grid}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} If bounds (low, high) are specified, use them to set axis limits}
             \PY{k}{if} \PY{n}{low} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{o+ow}{and} \PY{n}{high} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                 \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{low}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{high}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                 \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{n}{low}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{high}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Otherwise use first, last grid locations as low, high (for further mapping discretized samples)}
                 \PY{n}{low} \PY{o}{=} \PY{p}{[}\PY{n}{splits}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{splits} \PY{o+ow}{in} \PY{n}{grid}\PY{p}{]}
                 \PY{n}{high} \PY{o}{=} \PY{p}{[}\PY{n}{splits}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{splits} \PY{o+ow}{in} \PY{n}{grid}\PY{p}{]}
         
             \PY{c+c1}{\PYZsh{} Map each discretized sample (which is really an index) to the center of corresponding grid cell}
             \PY{n}{grid\PYZus{}extended} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{low}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{grid}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{high}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} add low and high ends}
             \PY{n}{grid\PYZus{}centers} \PY{o}{=} \PY{p}{(}\PY{n}{grid\PYZus{}extended}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{+} \PY{n}{grid\PYZus{}extended}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{l+m+mi}{2}  \PY{c+c1}{\PYZsh{} compute center of each grid cell}
             \PY{n}{locs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{n}{grid\PYZus{}centers}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{discretized\PYZus{}samples}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{grid}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}  \PY{c+c1}{\PYZsh{} map discretized samples}
         
             \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{samples}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{samples}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} plot original samples}
             \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{locs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{locs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} plot discretized samples in mapped locations}
             \PY{n}{ax}\PY{o}{.}\PY{n}{add\PYZus{}collection}\PY{p}{(}\PY{n}{mc}\PY{o}{.}\PY{n}{LineCollection}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{samples}\PY{p}{,} \PY{n}{locs}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{colors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} add a line connecting each original\PYZhy{}discretized sample}
             \PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{original}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{discretized}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
             
         \PY{n}{visualize\PYZus{}samples}\PY{p}{(}\PY{n}{samples}\PY{p}{,} \PY{n}{discretized\PYZus{}samples}\PY{p}{,} \PY{n}{grid}\PY{p}{,} \PY{n}{low}\PY{p}{,} \PY{n}{high}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now that we have a way to discretize a state space, let's apply it to
our reinforcement learning environment.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{c+c1}{\PYZsh{} Create a grid to discretize the state space}
         \PY{n}{state\PYZus{}grid} \PY{o}{=} \PY{n}{create\PYZus{}uniform\PYZus{}grid}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{low}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{high}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{state\PYZus{}grid}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}89}]:} array([[-1. , -0.8, -0.7, -0.5, -0.3, -0.1,  0.1,  0.2,  0.4],
                [-0.1, -0. , -0. , -0. ,  0. ,  0. ,  0. ,  0. ,  0.1]])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{c+c1}{\PYZsh{} Obtain some samples from the space, discretize them, and then visualize them}
         \PY{n}{state\PYZus{}samples} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         \PY{n}{discretized\PYZus{}state\PYZus{}samples} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{discretize}\PY{p}{(}\PY{n}{sample}\PY{p}{,} \PY{n}{state\PYZus{}grid}\PY{p}{)} \PY{k}{for} \PY{n}{sample} \PY{o+ow}{in} \PY{n}{state\PYZus{}samples}\PY{p}{]}\PY{p}{)}
         \PY{n}{visualize\PYZus{}samples}\PY{p}{(}\PY{n}{state\PYZus{}samples}\PY{p}{,} \PY{n}{discretized\PYZus{}state\PYZus{}samples}\PY{p}{,} \PY{n}{state\PYZus{}grid}\PY{p}{,}
                           \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{low}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{o}{.}\PY{n}{high}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{position}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{velocity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}  \PY{c+c1}{\PYZsh{} axis labels for MountainCar\PYZhy{}v0 state space}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    You might notice that if you have enough bins, the discretization
doesn't introduce too much error into your representation. So we may be
able to now apply a reinforcement learning algorithm (like Q-Learning)
that operates on discrete spaces. Give it a shot to see how well it
works!

\subsubsection{5. Q-Learning}\label{q-learning}

Provided below is a simple Q-Learning agent. Implement the
\texttt{preprocess\_state()} method to convert each continuous state
sample to its corresponding discretized representation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{k}{class} \PY{n+nc}{QLearningAgent}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Q\PYZhy{}Learning agent that can act on a continuous state space by discretizing it.\PYZdq{}\PYZdq{}\PYZdq{}}
         
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{env}\PY{p}{,} \PY{n}{state\PYZus{}grid}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.02}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.99}\PY{p}{,}
                          \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{epsilon\PYZus{}decay\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.9995}\PY{p}{,} \PY{n}{min\PYZus{}epsilon}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{01}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{505}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initialize variables, create grid for discretization.\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{c+c1}{\PYZsh{} Environment info}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env} \PY{o}{=} \PY{n}{env}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}grid} \PY{o}{=} \PY{n}{state\PYZus{}grid}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}size} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{splits}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1} \PY{k}{for} \PY{n}{splits} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}grid}\PY{p}{)}  \PY{c+c1}{\PYZsh{} n\PYZhy{}dimensional state space}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{o}{.}\PY{n}{n}  \PY{c+c1}{\PYZsh{} 1\PYZhy{}dimensional discrete action space}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Environment:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{State space size:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}size}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Action space size:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size}\PY{p}{)}
                 
                 \PY{c+c1}{\PYZsh{} Learning parameters}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha} \PY{o}{=} \PY{n}{alpha}  \PY{c+c1}{\PYZsh{} learning rate}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{gamma} \PY{o}{=} \PY{n}{gamma}  \PY{c+c1}{\PYZsh{} discount factor}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{initial\PYZus{}epsilon} \PY{o}{=} \PY{n}{epsilon}  \PY{c+c1}{\PYZsh{} initial exploration rate}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon\PYZus{}decay\PYZus{}rate} \PY{o}{=} \PY{n}{epsilon\PYZus{}decay\PYZus{}rate} \PY{c+c1}{\PYZsh{} how quickly should we decrease epsilon}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{min\PYZus{}epsilon} \PY{o}{=} \PY{n}{min\PYZus{}epsilon}
                 
                 \PY{c+c1}{\PYZsh{} Create Q\PYZhy{}table}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}size} \PY{o}{+} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size}\PY{p}{,}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Q table size:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{preprocess\PYZus{}state}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Map a continuous state to its discretized representation.\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{c+c1}{\PYZsh{} TODO: Implement this}
                 \PY{k}{pass}
         
             \PY{k}{def} \PY{n+nf}{reset\PYZus{}episode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Reset variables for a new episode.\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{c+c1}{\PYZsh{} Gradually decrease exploration rate}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon} \PY{o}{*}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon\PYZus{}decay\PYZus{}rate}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{min\PYZus{}epsilon}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} Decide initial action}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}state} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{preprocess\PYZus{}state}\PY{p}{(}\PY{n}{state}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{[}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}state}\PY{p}{]}\PY{p}{)}
                 \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}action}
             
             \PY{k}{def} \PY{n+nf}{reset\PYZus{}exploration}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Reset exploration rate used when training.\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon} \PY{o}{=} \PY{n}{epsilon} \PY{k}{if} \PY{n}{epsilon} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{initial\PYZus{}epsilon}
         
             \PY{k}{def} \PY{n+nf}{act}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{done}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Pick next action and update internal Q table (when mode != \PYZsq{}test\PYZsq{}).\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{n}{state} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{preprocess\PYZus{}state}\PY{p}{(}\PY{n}{state}\PY{p}{)}
                 \PY{k}{if} \PY{n}{mode} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} Test mode: Simply produce an action}
                     \PY{n}{action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} Train mode (default): Update Q table, pick next action}
                     \PY{c+c1}{\PYZsh{} Note: We update the Q table entry for the *last* (state, action) pair with current state, reward}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{[}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}state} \PY{o}{+} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}action}\PY{p}{,}\PY{p}{)}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha} \PY{o}{*} \PYZbs{}
                         \PY{p}{(}\PY{n}{reward} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{gamma} \PY{o}{*} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{[}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}state} \PY{o}{+} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}action}\PY{p}{,}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} Exploration vs. exploitation}
                     \PY{n}{do\PYZus{}exploration} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epsilon}
                     \PY{k}{if} \PY{n}{do\PYZus{}exploration}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} Pick a random action}
                         \PY{n}{action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size}\PY{p}{)}
                     \PY{k}{else}\PY{p}{:}
                         \PY{c+c1}{\PYZsh{} Pick the best action from Q table}
                         \PY{n}{action} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} Roll over current state, action for next step}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}state} \PY{o}{=} \PY{n}{state}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{last\PYZus{}action} \PY{o}{=} \PY{n}{action}
                 \PY{k}{return} \PY{n}{action}
         
             
         \PY{n}{q\PYZus{}agent} \PY{o}{=} \PY{n}{QLearningAgent}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{state\PYZus{}grid}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Environment: <TimeLimit<MountainCarEnv<MountainCar-v0>>>
State space size: (10, 10)
Action space size: 3
Q table size: (10, 10, 3)

    \end{Verbatim}

    Let's also define a convenience function to run an agent on a given
environment. When calling this function, you can pass in
\texttt{mode=\textquotesingle{}test\textquotesingle{}} to tell the agent
not to learn.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{k}{def} \PY{n+nf}{run}\PY{p}{(}\PY{n}{agent}\PY{p}{,} \PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{=}\PY{l+m+mi}{20000}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Run agent in given reinforcement learning environment and return scores.\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{max\PYZus{}avg\PYZus{}score} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{inf}
             \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Initialize episode}
                 \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
                 \PY{n}{action} \PY{o}{=} \PY{n}{agent}\PY{o}{.}\PY{n}{reset\PYZus{}episode}\PY{p}{(}\PY{n}{state}\PY{p}{)}
                 \PY{n}{total\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{done} \PY{o}{=} \PY{k+kc}{False}
         
                 \PY{c+c1}{\PYZsh{} Roll out steps until done}
                 \PY{k}{while} \PY{o+ow}{not} \PY{n}{done}\PY{p}{:}
                     \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
                     \PY{n}{total\PYZus{}reward} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
                     \PY{n}{action} \PY{o}{=} \PY{n}{agent}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{mode}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} Save final score}
                 \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{total\PYZus{}reward}\PY{p}{)}
                 
                 \PY{c+c1}{\PYZsh{} Print episode stats}
                 \PY{k}{if} \PY{n}{mode} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{100}\PY{p}{:}
                         \PY{n}{avg\PYZus{}score} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{100}\PY{p}{:}\PY{p}{]}\PY{p}{)}
                         \PY{k}{if} \PY{n}{avg\PYZus{}score} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}avg\PYZus{}score}\PY{p}{:}
                             \PY{n}{max\PYZus{}avg\PYZus{}score} \PY{o}{=} \PY{n}{avg\PYZus{}score}
         
                     \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ | Max Average Score: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{p}{,} \PY{n}{max\PYZus{}avg\PYZus{}score}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                         \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{flush}\PY{p}{(}\PY{p}{)}
         
             \PY{k}{return} \PY{n}{scores}
         
         \PY{n}{scores} \PY{o}{=} \PY{n}{run}\PY{p}{(}\PY{n}{q\PYZus{}agent}\PY{p}{,} \PY{n}{env}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        TypeError                                 Traceback (most recent call last)

        <ipython-input-79-1ccce04e3018> in <module>()
         32     return scores
         33 
    ---> 34 scores = run(q\_agent, env)
    

        <ipython-input-79-1ccce04e3018> in run(agent, env, num\_episodes, mode)
         14             state, reward, done, info = env.step(action)
         15             total\_reward += reward
    ---> 16             action = agent.act(state, reward, done, mode)
         17 
         18         \# Save final score
    

        <ipython-input-78-013786e03269> in act(self, state, reward, done, mode)
         55             \# Train mode (default): Update Q table, pick next action
         56             \# Note: We update the Q table entry for the *last* (state, action) pair with current state, reward
    ---> 57             self.q\_table[self.last\_state + (self.last\_action,)] += self.alpha *                 (reward + self.gamma * max(self.q\_table[state]) - self.q\_table[self.last\_state + (self.last\_action,)])
         58 
         59             \# Exploration vs. exploitation
    

        TypeError: unsupported operand type(s) for +: 'NoneType' and 'tuple'

    \end{Verbatim}

    The best way to analyze if your agent was learning the task is to plot
the scores. It should generally increase as the agent goes through more
episodes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Plot scores obtained per episode}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Scores}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    If the scores are noisy, it might be difficult to tell whether your
agent is actually learning. To find the underlying trend, you may want
to plot a rolling mean of the scores. Let's write a convenience function
to plot both raw scores as well as a rolling mean.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{plot\PYZus{}scores}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{rolling\PYZus{}window}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Plot scores and optional rolling mean using specified window.\PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{;} \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Scores}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{;}
            \PY{n}{rolling\PYZus{}mean} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{o}{.}\PY{n}{rolling}\PY{p}{(}\PY{n}{rolling\PYZus{}window}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{rolling\PYZus{}mean}\PY{p}{)}\PY{p}{;}
            \PY{k}{return} \PY{n}{rolling\PYZus{}mean}
        
        \PY{n}{rolling\PYZus{}mean} \PY{o}{=} \PY{n}{plot\PYZus{}scores}\PY{p}{(}\PY{n}{scores}\PY{p}{)}
\end{Verbatim}


    You should observe the mean episode scores go up over time. Next, you
can freeze learning and run the agent in test mode to see how well it
performs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Run in test mode and analyze scores obtained}
        \PY{n}{test\PYZus{}scores} \PY{o}{=} \PY{n}{run}\PY{p}{(}\PY{n}{q\PYZus{}agent}\PY{p}{,} \PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[TEST] Completed }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ episodes with avg. score = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plot\PYZus{}scores}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{,} \PY{n}{rolling\PYZus{}window}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


    It's also interesting to look at the final Q-table that is learned by
the agent. Note that the Q-table is of size MxNxA, where (M, N) is the
size of the state space, and A is the size of the action space. We are
interested in the maximum Q-value for each state, and the corresponding
(best) action associated with that value.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k}{def} \PY{n+nf}{plot\PYZus{}q\PYZus{}table}\PY{p}{(}\PY{n}{q\PYZus{}table}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Visualize max Q\PYZhy{}value for each state and corresponding action.\PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{q\PYZus{}image} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{q\PYZus{}table}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}       \PY{c+c1}{\PYZsh{} max Q\PYZhy{}value for each state}
            \PY{n}{q\PYZus{}actions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{q\PYZus{}table}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} best action for each state}
        
            \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
            \PY{n}{cax} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{q\PYZus{}image}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
            \PY{n}{cbar} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{n}{cax}\PY{p}{)}
            \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{q\PYZus{}image}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{q\PYZus{}image}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                    \PY{n}{ax}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{q\PYZus{}actions}\PY{p}{[}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                            \PY{n}{horizontalalignment}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{verticalalignment}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Q\PYZhy{}table, size: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{q\PYZus{}table}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{position}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{velocity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        
        \PY{n}{plot\PYZus{}q\PYZus{}table}\PY{p}{(}\PY{n}{q\PYZus{}agent}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{)}
\end{Verbatim}


    \subsubsection{6. Modify the Grid}\label{modify-the-grid}

Now it's your turn to play with the grid definition and see what gives
you optimal results. Your agent's final performance is likely to get
better if you use a finer grid, with more bins per dimension, at the
cost of higher model complexity (more parameters to learn).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} TODO: Create a new agent with a different state space grid}
        \PY{n}{state\PYZus{}grid\PYZus{}new} \PY{o}{=} \PY{n}{create\PYZus{}uniform\PYZus{}grid}\PY{p}{(}\PY{err}{?}\PY{p}{,} \PY{err}{?}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{p}{(}\PY{err}{?}\PY{p}{,} \PY{err}{?}\PY{p}{)}\PY{p}{)}
        \PY{n}{q\PYZus{}agent\PYZus{}new} \PY{o}{=} \PY{n}{QLearningAgent}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{state\PYZus{}grid\PYZus{}new}\PY{p}{)}
        \PY{n}{q\PYZus{}agent\PYZus{}new}\PY{o}{.}\PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}  \PY{c+c1}{\PYZsh{} initialize a list to store scores for this agent}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Train it over a desired number of episodes and analyze scores}
        \PY{c+c1}{\PYZsh{} Note: This cell can be run multiple times, and scores will get accumulated}
        \PY{n}{q\PYZus{}agent\PYZus{}new}\PY{o}{.}\PY{n}{scores} \PY{o}{+}\PY{o}{=} \PY{n}{run}\PY{p}{(}\PY{n}{q\PYZus{}agent\PYZus{}new}\PY{p}{,} \PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{=}\PY{l+m+mi}{50000}\PY{p}{)}  \PY{c+c1}{\PYZsh{} accumulate scores}
        \PY{n}{rolling\PYZus{}mean\PYZus{}new} \PY{o}{=} \PY{n}{plot\PYZus{}scores}\PY{p}{(}\PY{n}{q\PYZus{}agent\PYZus{}new}\PY{o}{.}\PY{n}{scores}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Run in test mode and analyze scores obtained}
        \PY{n}{test\PYZus{}scores} \PY{o}{=} \PY{n}{run}\PY{p}{(}\PY{n}{q\PYZus{}agent\PYZus{}new}\PY{p}{,} \PY{n}{env}\PY{p}{,} \PY{n}{num\PYZus{}episodes}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[TEST] Completed }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ episodes with avg. score = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plot\PYZus{}scores}\PY{p}{(}\PY{n}{test\PYZus{}scores}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Visualize the learned Q\PYZhy{}table}
        \PY{n}{plot\PYZus{}q\PYZus{}table}\PY{p}{(}\PY{n}{q\PYZus{}agent\PYZus{}new}\PY{o}{.}\PY{n}{q\PYZus{}table}\PY{p}{)}
\end{Verbatim}


    \subsubsection{7. Watch a Smart Agent}\label{watch-a-smart-agent}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{state} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
        \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{)}\PY{p}{:}
            \PY{n}{action} \PY{o}{=} \PY{n}{q\PYZus{}agent\PYZus{}new}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{env}\PY{o}{.}\PY{n}{render}\PY{p}{(}\PY{p}{)}
            \PY{n}{state}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{done}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}
            \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
            \PY{k}{if} \PY{n}{done}\PY{p}{:}
                \PY{k}{break} 
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{score}\PY{p}{)}
        \PY{n}{env}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
